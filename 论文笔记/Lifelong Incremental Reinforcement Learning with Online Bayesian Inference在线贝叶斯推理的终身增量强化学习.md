##  论文笔记：《Lifelong Incremental Reinforcement Learning with Online Bayesian Inference》

中译：在线贝叶斯推理的终身增量强化学习

总结：
怎么使用贝叶斯推理和EM算法的？
模型有两个参数，一个学习参数，一个环境参数

## ●Abstract

-   question作者想解决什么问题？

    终身学习好，可以应用到增量强化学习中
-   method作者通过什么理论/模型来解决这个问题？

    提出了**终身增量强化学习**(LLIRL)，这是一种新的增量算法，用于有效地终身适应动态环境。我们开发了一个**参数化环境模型**的定义混合，相当于将环境参数聚集在一个潜在的空间中。混合先验分布被公式化为一个中国餐馆过程(CRP)，它增量地实例化新的环境模型，而不需要任何外部信息来提前发出环境变化的信号。<font color=red>在终身学习过程中，我们采用**在线贝叶斯**推理的**期望最大化**算法以完全增量的方式更新混合。在EM中，E步涉及估计环境到聚类分配的后验期望，而M步更新环境参数用于未来的学习。</font> 这种方法允许根据需要调整所有环境模型，新模型为环境变化进行实例化，旧模型在再次遇到以前看到的环境时进行检索。
-   answer作者给出的答案是什么？

	 该方法优于现有的相关方法，能够有效地适应各种动态环境，实现终身学习
    

## ●Instruction

-   why作者为什么研究这个课题
    
    开发一种新的增量学习算法，用于终身适应动态环境。
-   how当前研究到了哪一阶段
    
    **开发并维护**一个包含无限个成对参数的库。一个是用于学习行为策略的规范“**学习参数**”，例如直接策略搜索中的策略网络[11]。另一种称为“**环境参数**”，是使用任意函数逼近器(如神经网络)对环境进行参数化，该函数逼近器可以实例化为奖励或状态转移函数。为了处理随时间变化的**动态环境分布**，<font color=red>我们在环境参数上引入了无限混合的**贝叶斯模型**，这相当于在潜在空间中对环境参数进行**聚类**。</font> 

	在终身学习过程中，我们采用带在线贝叶斯推理的**期望最大化**算法，以完全增量的方式更新混合环境模型。进化算法中的**进化**步骤对应于计算环境概率的**后验**推断，而进化算法中的进化步骤适用于为将来的学习逐步更新环境参数。这使得所有环境模型都可以根据需要进行调整，**新模型**可以根据环境变化进行实例化，**旧模型**可以在再次遇到以前看到的环境时进行检索。
	（进化算法的目标与强化学习优化的目标都是预期奖励。但是，强化学习是将噪声注入动作空间并使用反向传播来计算参数更新，而进化策略则是直接向参数空间注入噪声。换个说话，强化学习是在「猜测然后检验」动作，而进化策略则是在「猜测然后检验」参数。因为我们是在向参数注入噪声，所以就有可能使用确定性的策略（而且我们在实验中也确实是这么做的）。也有可能同时将噪声注入到动作和参数中，这样就有可能实现两种方法的结合。from：[深度强化学习中的进化算法总结](https://zhuanlan.zhihu.com/p/189010215)）
-   what作者基于什么样的假设（看不懂最后去查）
    

## ●Conclusion

-   优点
    
    在无限混合之前的CRP使得新的环境模型能够根据需要被增量地实例化，而不需要任何外部信息来提前发出环境变化的信号。
-   Future

	虽然我们使用策略梯度作为我们的评估域，但我们的方法是通用的，可以很容易地在其他RL架构上实现(例如，深度Q网络[9])。未来工作的一个潜在方向是采用非参数模型(如高斯过程、k近邻)来更灵活地表示环境。另一个方向是开发一个高效的框架，只引入一组参数来同时训练策略和参数化环境。
    

## ●Table & Method

-   数据来源
    
-   重要指标
    -  所有迭代的平均回报方面的数值结果（ 2D Navigation导航任务）（人话：迭代次数的平均值（因为终身学习所以能尽快迭代到收敛））
    - 集群数量的影响：不同问题有不同的最优集群数量
-   模型步骤 + 每个步骤得出的结论

	- 将环境实例化参数化，然后将以前看到的环境聚集在一个潜在的空间中，减少存储库中的冗余。
	- 开发并维护一个库，该库在动态环境中的终身学习过程中包含潜在无限数量的成对参数(θ(∞),ϑ(∞):θ用于学习行为策略(例如，策略网络)，ϑ用于参数化环境(例如，由神经网络近似的奖励或状态转移函数)。
	- 终身增量强化学习(LLIRL)算法
	![](Lifelong%20Incremental%20Reinforcement%20Learning%20with%20Online%20Bayesian%20Inference%E5%9C%A8%E7%BA%BF%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E7%90%86%E7%9A%84%E7%BB%88%E8%BA%AB%E5%A2%9E%E9%87%8F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0_md_files/image.png?v=1&type=image)
	1. 初始化$θ,\vartheta$潜在环境聚类
	2. 从统一行为策略中取样得到$T_\varepsilon$
	3. 计算环境模型样本$X$，$Y$
	4. 环境模型在样本上的预测可能性$P_\vartheta(Y|X)$
	5. 环境到聚类分配的后验$P(\vartheta|X,Y)$
	6. 当$P(\vartheta^{(L+1)}|X,Y) > P(\vartheta^l|X,Y)$，更新$θ,\vartheta$
	7. 执行EM过程直到收敛（E：计算$P(\vartheta^l|X,Y)$；M：使用$P(\vartheta^l|X,Y)$计算$\vartheta$）
	8. 为未来的学习保留$\vartheta$，计算l，用$θ$和l的环境下学习，直到收敛，更新$θ$参数
