##  论文笔记：《Regularize, Expand and Compress Multi-task based Lifelong Learning via NonExpansive AutoML》
中译：正则化、扩展和压缩基于多任务的非扩张式自动机器学习

总结：通过任务相关性为任务分配权重，使用自动学习扩展网络获得新任务的权重，然后压缩模型到第一个任务的模型大小。

## ●Abstract

-   question作者想解决什么问题？
    深度神经网络的终身学习有两个障碍:灾难性遗忘和容量限制。
-   method作者通过什么理论/模型来解决这个问题？
    受最近在**自动学习良好神经网络体系结构**方面的突破的启发，我们通过称为**正则化、扩展和压缩(REC)的非扩展自动学习框架**开发了一个基于多任务的终身学习
    
	REC由三个阶段组成:
	- 1)通过一种新提出的**多任务加权合并**(MWC)算法，在没有已学习任务数据的情况下连续学习连续任务；
	- 2)通过基于网络转换的**自动学习**，扩展网络以帮助终身学习，潜在地提高模型能力和性能；
	- 3)在学习每个新任务后**压缩扩展的模型**，以保持模型的效率和性能。
-   answer作者给出的答案是什么？
    MWC和REC优于其他终身学习算法的性能。

## ●Instruction

-   why作者为什么研究这个课题
    终身学习的主要目标是在不忘记从先前训练的任务中学习的知识的情况下学习连续的任务，并利用先前的知识在新到来的任务上获得更好的性能或更快的收敛。一个简单的方法是**为每个新任务微调模型**；然而，这种再训练通常在新的任务和旧的任务上，模型的性能都会退化。如果新任务与旧任务有很大不同，它可能无法学习新任务的最佳模型。重新训练的表示可能导致**灾难性遗忘**[^1]。
    
    如果我们在**终身学习**环境中直接使用**自动学习**，它会忘记旧任务的知识，并且是一个浪费的过程，因为控制器需要从头开始搜索每个新的任务网络架构，忽略以前学习的任务和新任务之间的相关性。
    [^1]:同时，重新训练的表示可能对旧任务产生不利影响，导致它们偏离最优解。这可能会导致“**灾难性遗忘**”，这是一种用新任务训练模型的现象，会干扰先前学习的旧知识，导致性能下降，甚至用新知识覆盖旧知识。
    [^2]:自动学习是指为给定的任务自动学习合适的机器学习模型——神经体系结构搜索[32]是用于深度学习的自动学习的一个子领域，它使用强化学习来搜索设计网络体系结构的最佳超参数。RL框架有一个主控制器，它观察生成的子网络在验证集上的性能作为奖励信号，然后它给性能较高的体系结构以较高的概率来更新模型。
-   how当前研究到了哪一阶段
    提出了一种多任务权重合并(MWC)方法，通过合并旧任务和新任务之间的内在相关性来学习区分性**权重**子集。此外，为了缩小结构搜索空间和节省训练时间，利用基**于网络转换的自动学习来加速新网络的元学习**。

	然而，如果我们不断为越来越多的新任务**扩展网络**，与初始模型相比，模型将具有大得多的模型大小，并且会遇到**低效**的问题(例如，低内存占用、低功耗)。许多基于网络扩展的终身学习算法增加了模型能力，但也降低了学习效率，如**内存成本和功耗**。为了解决这个问题，我们在完成每个新任务的学习后进行模型压缩——我们将扩展的模型**压缩到初始模型**，在新旧任务上的性能损失可以忽略不计。
-   what作者基于什么样的假设（看不懂最后去查）
    

## ●Conclusion

-   优点
    比其他终身学习方法更好的准确性和更小的模型大小
-   缺点
    计划减少基于自动学习算法的训练时间，并探索计算奖励样本的需求，以改进当前的工作。

## ●Table & Method

-   数据来源
    终身学习最常用的数据集上评估我们的算法：MNIST-permutation，MNIST-Variation，CIFAR-100，CUB-200，Base network settings，
-   重要指标
    模型精度（ACC）和模型复杂性（模型参数个数）
-   模型步骤 + 每个步骤得出的结论
	REC分两个阶段实现:**连续网络扩展和模型压缩**，并提出了一种新的多任务权重合并算法来克服灾难性遗忘。
	算法1：REC（正则化、扩展和压缩）
	![](Regularize,%20Expand%20and%20Compress%20Multi-task%20based%20Lifelong%20Learning%20via%20NonExpansive%20AutoML%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E6%89%A9%E5%B1%95%E5%92%8C%E5%8E%8B%E7%BC%A9%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9D%9E%E6%89%A9%E5%BC%A0%E5%BC%8F%E8%87%AA%E5%8A%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_md_files/image1.png?v=1&type=image)
	- if t==1: 使用等式1训练权重为$θ^1$的初始网络
	- else: 通过算法2计算最佳子网络。压缩$θ^t$到$θ^1$的大小，返回压缩后的θ。
	
	算法2：Automatically Network Transformation：自动网络转换（基于网络转换的REC自动学习的细节）
![](Regularize,%20Expand%20and%20Compress%20Multi-task%20based%20Lifelong%20Learning%20via%20NonExpansive%20AutoML%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E6%89%A9%E5%B1%95%E5%92%8C%E5%8E%8B%E7%BC%A9%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9D%9E%E6%89%A9%E5%BC%A0%E5%BC%8F%E8%87%AA%E5%8A%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_md_files/image2.png?v=1&type=image)
