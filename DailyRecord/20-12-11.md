# 20.12.11
目录：
 - 在线学习 Online Learning
 - 《An Overview of Multi-Task Learningin Deep Neural Networks》多任务学习综述

## 在线学习 Online Learning
[Online Learning算法理论与实践](https://tech.meituan.com/2016/04/21/online-learning.html)：

什么是Online Learning：Online Learning并不是一种模型，而是一种**模型的训练方法**，Online Learning能够根据线上反馈数据，实时快速地进行模型调整，使得模型及时反映线上的变化，提高线上预测的准确率。Online Learning的流程包括：**将模型的预测结果展现给用户，然后收集用户的反馈数据，再用来训练模型**，形成闭环的系统。

主流有FTRL（Follow The Regularized Leader，谷歌提出）和BPR（Bayesian Probit Regression，贝叶斯概率回归）。FTL（Follow THE Leader）的目标是损失函数的和最小，在FTL基础上加入正则化以防止过拟合，成为FTRL。BOL（贝叶斯在线学习）将后验结果作为下一次预测的先验，后验结果中就可以应用用户的反馈，BPR属于BOL的一种。

[在线算法和离线算法的概念](https://www.cnblogs.com/daiyl0320/articles/3344796.html)：这里的理解是，在线和离线的区别在于输入是否在一开始就确定。

## 《An Overview of Multi-Task Learningin Deep Neural Networks》多任务学习综述（17年）
11-23看过一个写这个的笔记，今天又看到了。补充一些。参考：[Multi-task Learning(Review)多任务学习概述](https://www.jianshu.com/p/93dc48dff59e)（这篇排版很乱，而且章的标题和某些翻译也是错的，尽量结合原文看）
### Chapter 3 MTL for DL
硬参数共享，软参数共享
### Chapter 4 Why does MTL work（工作机制）
### Chapter 5 非神经网络里的MTL
贯穿多任务学习历史的两个主要思想:通过范数正则化加强任务间的稀疏性；以及模拟任务之间的关系。
5.1 块稀疏正则化
- 结合使用 l1/lq范式

5.2 学习任务间关系
- 任务间关系较弱时，较强的关系可能会导致negative transfer负向效果，先聚类，使用同一类的学习。
- 通过先验控制多任务的相似，贪心地选择样本。
- 对每个task-specific layers 使用高斯先验，使用一个cluster的混合分布来促使不同任务的相似。
- 学习潜在的任务结构 。
- 假设两个不同cluster的两个任务之间存在重叠，存在一部分的latent basis tasks。令每个任务的模型参数是latent basis tasks的线性组合，对latent basis tasks限制为稀疏的。重叠部分控制共享程度

### Chapter 6 近期MTL研究
Deep Relation Network：计算机视觉中，一般**共享卷积层，之后是任务特定的DNN层**。

Fully-Adaptive Feature Sharing：从一个简单结构开始，**贪心地动态地加宽模型**，使相似的模型聚簇。

Cross-stitch Networks：通过**线性组合学习前一层的输出**（模型流程是从左往右），允许模型决定不同任务之间的分享程度。

### 可研究方向

1.  learning what to share
2.  measurement for similarity of tasks
3.  using task uncertainty
4.  引入异步任务（特征学习任务），采用交替迭代训练
5.  学习抽象子任务；学习任务结构（类似强化里面的hierarchy learning）
6.  参数学习辅助任务
7.  More...
