# 20.12.04
目录：
 - 伯努利
 - 【机器学习】确定最佳聚类数目的10种方法
 - 先降维再聚类的讨论
 - 《稀疏子空间聚类综述》by 王卫卫等
 - 低秩矩阵

## 伯努利
[beta分布及共轭Bernoulli分布-先验、后验、预测分布](https://blog.csdn.net/qy20115549/article/details/53307535)
![](21-01-04_md_files/image1.png?v=1&type=image)

## 【机器学习】确定最佳聚类数目的10种方法
https://www.cnblogs.com/think90/p/7133753.html
末尾还有总结。但是计算量都挺大的，要么是需要多次运算，要么是不准确。

## 先降维再聚类的讨论
[在做聚类分析的时候，因为属性维数比较大，是否需要先进行pca进行降维呢？](https://www.zhihu.com/question/30659703)，[问题2](https://www.applysquare.com/topic-cn/RaHJ2GGrj/)：
 - 效果不好，因为聚类的相似度基于距离，降维之后可能就损失了很多信息
 - 这个问题的难易程度(甚至可解与否)取决于这30个变量中有多少个跟cluster结构相关以及有多相关
 - 可以尝试
 - 使用稀疏子空间聚类
 
##   《稀疏子空间聚类综述》by 王卫卫等
自动化学报.2015.

1. 定义1(子空间聚类)给定一组数据X=[x1，x2，.，xN]∈R^(D*N)，设这组数据属于k(k已知或未知)个线性
子空间的并，子空间聚类是指将这组数据分割为不同的类，在理想情况下，每一个类对应一个子空间。
2. 子空间聚类算法可以粗略分为5类：基于矩阵分解的方法，代数方法，迭代方法，统计方法，谱聚类
3. Elhamifar 等[32]基于一维稀疏性提出了稀疏子空间聚类 (Sparse subspace clustering,SSC) 方法.Liu利用二维稀疏性提出了基于低秩表示 (LRR) 的子空间聚类方法,SSC和 LRR 是稀疏子空间聚类的先驱工作, 在数据聚类应用中取得了良好的效果.
4. 作者尝试用PCA降维，然后进行子空间聚类，在部分算法上还行，在部分算法上错误率提高很多
5. 利用隐空间（latent space），或基于矩阵分解理论使用低秩子空间聚类，可以提高稀疏子空间聚类的速度。Peng 等[84] 提出了可扩展稀疏子空间聚类 (Scalable sparse subspace clustering, SSSC) 方法, 采用 “采样、聚类、编码和分类” 的策略. 同样, 对于低秩子空间聚类也提出了可扩展方法[85]

## 低秩矩阵

 - [矩阵低秩的意义? - Towser的回答 - 知乎](https://www.zhihu.com/question/28630628/answer/80090557) ：而**稀疏性**的意思是（以稀疏表示为例），任给一个图像，**字典可能是过完备的**，从而用字典里的基向量表出这幅图有很多种不同的方案。我们希望**选取使用基底数量最少的那种方案**，因为简单的总是好的。往往对向量谈稀疏，矩阵谈低秩（其实矩阵低秩也是在一组特殊的基底下稀疏）
 - [矩阵低秩的意义? - Rebecca Lyu的回答 - 知乎](https://www.zhihu.com/question/28630628/answer/88043360) ：矩阵中的最大的不相关的向量的个数，就叫秩，可以理解为有秩序的程度。满秩则关系最清晰，低秩则关系错综复杂（不太懂）
 - [矩阵低秩的意义? - 董理的回答 - 知乎](https://www.zhihu.com/question/28630628/answer/80128827)：从应用上，不严谨地讲，低秩表征着一种**冗余程度**。  秩越低表示数据冗余性越大，因为用很少几个基就可以表达所有数据了。相反，秩越大表示数据冗余性越小。
 -  [矩阵低秩分解理论ppt](https://wenku.baidu.com/view/7128ca3014791711cc791765.html)
 -  [低秩分解](https://www.cnblogs.com/missidiot/p/9869182.html)：SVD分解，张量分解
 -  [低秩表示的学习--Latent Low-Rank Representation(LatLLR)](https://blog.csdn.net/zzq060143/article/details/89876694)
 -  [深度学习之模型压缩](https://www.cnblogs.com/jimchen1218/p/11957885.html)：目的：降低时间复杂度；优点：适合模型压缩；缺点：实现并不容易，涉及计算成本高昂的分解操作，且需要大量的重新训练来达到收敛。
 - [奇异值分解(SVD)原理及应用](https://www.cnblogs.com/tianqizhi/p/9745913.html)：奇异值的计算是O(N^3)的算法，matlab在一秒钟内就可以算出1000 * 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。Google实现了SVD的并行化算法，另，可使用Map-Reduce并行。】从直观上我们发现U矩阵和V矩阵可以近似来代表A矩阵，换据话说就是将A矩阵压缩成U矩阵和V矩阵，至于压缩比例得看当时对S矩阵取前k个数的k值是多少。

![](21-01-04_md_files/image2.png?v=1&type=image)

