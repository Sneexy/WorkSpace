# 21.01.08
目录：
 - 增强学习和有/半/无监督学习的区别
 - 贝叶斯优化
 - 期望最大化（EM）算法
 - 流形（manifold）
 - 经验重放（experience replay）


## 增强学习和有/半/无监督学习的区别
[有监督学习、无监督学习以及强化学习](https://zhuanlan.zhihu.com/p/26304729)
[有/无/半监督学习 强化学习 区别](https://www.jianshu.com/p/34f4d31dd2fc)

## 贝叶斯优化
- [贝叶斯优化:一种更好的超参数调优方式](https://blog.csdn.net/u010159842/article/details/83030571)
-  [对朴素贝叶斯法后验概率最大化含义的一些思考](https://blog.csdn.net/yaokun2012/article/details/81913129)
- [概率图模型基础 - 贝叶斯网络参数学习（贝叶斯估计+碎权更新法）](https://blog.csdn.net/Snoopy_Yuan/article/details/66477014)（和要考虑的没关系）
	
- [为什么基于贝叶斯优化的自动调参没有大范围使用？](https://www.zhihu.com/question/33711002)

	[微调的回答 - 知乎](https://www.zhihu.com/question/33711002/answer/1437949092) ：首先是维度的问题，BO处理的参数维度一般默认是在**20维以内**，这个本身会限制可以优化的参数类型。对于**无监督学习**，或者没有通用目标函数的场景，BO也就不可用了。比如是无监督的，我们也可以引入人来进行评估，那么其中可能还有不少主观上的bias加进去。
	
	 [刘斯坦的回答 - 知乎](https://www.zhihu.com/question/33711002/answer/1536396538) ：调参后模型耗时长，且有不确定因素，玄学

	 [若羽的回答 - 知乎](https://www.zhihu.com/question/33711002/answer/1435788341) ：1.容易陷入局部最优，虽然这个可以用躲避老虎机的Exploration And Exploitation解决。  2.依赖暴力美学，算力要求巨大。  3.高维灾难，对非凸优化不友善。  4.效果不稳定，有论文表明，贝叶斯优化算法并不显著优于随机搜索(random search)。不能大规模使用的很大一个原因，无非是，你无法对于为生计而奔波的人推广海景别墅。
	 
	[云对雨的回答 - 知乎](https://www.zhihu.com/question/33711002/answer/232910262) ：对于学术界来说，靠调参刷出的结果不好发paper啊，还是要研究下新颖的模型结构。对于工业界来说，数据量是第一位的，其次直接套成熟的大模型了，后面才是研究调参，这种事肯定还是有公司在做的，只不过公司也不会用来宣传吧。。

	[抽象猴的回答 - 知乎](https://www.zhihu.com/question/33711002/answer/80162506) ：深度學習自個有實時自適應調參算法, AdaGrad, AdaDelta等, 貝葉斯優化還得訓練完一整回才能調一次參數, 響應來說很差. 另有提到貝葉斯優化網路拓樸等, 諸如pooling的寬度, 這點貌似可以關注. 但若以後出現自適應增減網路拓樸的算法, 貝葉斯優化就雞肋了. 投注這塊不合算
- [自动机器学习超参数调整（贝叶斯优化）](https://www.cnblogs.com/wmx24/p/10025600.html)
	- Python中的选择：Python中有几个贝叶斯优化库，它们目标函数的替代函数不一样。在本文中，我们将使用**Hyperopt**，它使用Tree Parzen Estimator（TPE）。其他Python库包括Spearmint（高斯过程代理）和SMAC（随机森林回归）。
	- 优化问题的四个部分：
		-   目标函数：我们想要最小化的内容，在这里，目标函数是机器学习模型使用该组超参数在验证集上的损失。
		-   域空间：要搜索的超参数的取值范围   
		-   优化算法：构造替代函数并选择下一个超参数值进行评估的方法。 
		-   结果历史记录：来自目标函数评估的存储结果，包括超参数和验证集上的损失。

## 期望最大化（EM）算法
- [机器学习算法/模型——有监督到无监督（聚类）：由 KNN 到 K-menas](https://blog.csdn.net/Robin_Pi/article/details/104490668/)：K-means是EM算法的一个非常简单并且易于理解的应用。
- [EM算法--期望最大化算法](https://www.cnblogs.com/chenweitian/p/7810506.html)（Q：无固定参数可用吗）
E-步：计算完整数据的对数似然函数的期望，记为：  

	Q(Θ|Θ (t)) = E{Lc(Θ;Z)|X;Θ(t)}；  

	M-步：通过最大化Q(Θ|Θ(t) ) 来获得新的Θ 。  

	通过交替使用这两个步骤，EM算法逐步改进模型的参数，使参数和训练样本的似然概率逐渐增大，最后终止于一个极大点。直观地理解EM算法，它也可被看作为一个逐次逼近算法：事先并不知道模型的参数，可以随机的选择一套参数或者事先粗略地给定某个初始参数λ0 ，确定出对应于这组参数的最可能的状态，计算每个训练样本的可能结果的概率，在当前的状态下再由样本对参数修正，**重新估计参数λ**，并在新的参数下重新确定模型的状态，这样，通过多次的迭代，循环直至某个收敛条件满足为止，就可以使得模型的参数逐渐**逼近真实参数**。  

	EM算法的主要目的是提供一个简单的迭代算法计算后验密度函数，它的最大优点是简单和稳定，但容易陷入局部最优。
-  [EM算法理解的九层境界](https://www.cnblogs.com/wqbin/p/11080402.html)
-  [EM算法（期望最大化）——应用：GMM](https://blog.csdn.net/tingyue_/article/details/70576025)（GMM（Gaussian Mixture Model）也叫高斯混合模型）
- [期望最大化(EM)算法真如用起来那么简单？](https://zhuanlan.zhihu.com/p/25799397)

## 流形（manifold）
- [流形（manifold）介绍](https://blog.csdn.net/qq_30545831/article/details/81776819)
	流形是一种空间，一个流形好比是一个 d 维的空间，在一个 m 维的空间中 (m > d) 被扭曲之后的结果（一般**维度压缩**的方法中都会提到这个词，谱聚类中就有涉及这个思想，稍后再说），可以类似于地球，地球的表面是一个球面。

## 经验重放（experience replay）
- 论文《Experience Replay for Continual Learning》在zotero中，但是它说的是一般认为重放可以有好的效果，但是需要有策略地重放才行，这个策略有点复杂没仔细看。另使用这个的我看到的都是强化学习的。
- [Experience Replay for Continual Learning-知乎](https://zhuanlan.zhihu.com/p/143955414)
- [经验回放（Experience replay）](https://blog.csdn.net/suoyan1539/article/details/79571010)
- [从NLP终生学习开始，谈谈深度学习中记忆结构的设计和使用](https://cloud.tencent.com/developer/article/1507000)：各种NLP中的经验重放使用，只有一个没说是增强学习。感觉这种经验重放的策略是通过存储某些样本在记忆中的，可能还是与长短时记忆有关，《Efficient Meta Lifelong-Learning with Limited Memory》记录了样本还能算终身学习吗，也许是不纯粹的终身学习
- ![](21-01-08_md_files/image.png?v=1&type=image)
