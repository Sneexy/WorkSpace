# 20.11.23日志
目录：
 1. 多任务学习-看博客
 2. VS 2019控制台输出中文乱码问题

## 多任务学习：
继续看博客：[深度神经网络中的多任务学习汇总](https://zhuanlan.zhihu.com/p/52566508)，下面是笔记摘录

### 3. 深度学习中的两种多任务学习模式
参数的硬共享：共享隐层，过拟合风险小
![preview](https://pic4.zhimg.com/v2-52328d5106801262194a2aa8c4834a67_r.jpg)

参数的软共享：每个任务都有自己的模型，自己的参数，相互约束
![preview](https://pic4.zhimg.com/v2-377747242f66b1d0d1f008f97e8a8257_r.jpg)

### 5. 非神经网络中的多任务学习
对任务之间的不同强制添加稀疏性约束的正则化项
建模任务之间的关系

### 6. 深度神经网络中多任务学习的进展
6.1 深度关系网络(Deep Relationship Networks)，共享卷积层，模型可以学习任务间关系，预定义共享结构，可能对新任务有错误倾向

6.2 完全自适应特征共享(Fully-Adaptive Feature Sharing)，对相似任务自动分组，贪心，无任务间学习

6.3 十字绣网络(Cross-Stitch Networks)，将两个独立的网络用参数软共享的方式连接起来

6.6 用不确定性对损失进行加权(Weighting Losses with Uncertainty)

6.9 模型应该共享些什么（研究方向）
处理不相关任务：
 - 任务的层次结构
 - 任务间本应该存在的交互模式

###  7. 辅助任务
由于多任务学习中，大多数情况仅关注一个任务，为此其他任务就需要辅助好它
7.1 相关任务(Related Tasks)

7.2 对抗任务(Adversarial Tasks)
当目标损失不便于最小化时，对抗任务与目标相反，让对抗任务损失最大化。

7.3 提示性任务(Hint Tasks)
使用提示机制就是在辅助任务中预测特征

7.4 注意力集中(Focusing Attention)
关注某一个部分的任务

7.5 量化平滑(Quantization Smoothing)
当标注是离散的，优化目标是被量化的，使用量化平滑使目标函数变成连续型

7.6 预测输入(Predicting Inputs)
某些用不到的数据作为输出来监督学习过程

7.7 用未来预测现在(Using the Future to Predict the Present)
使用预测结果来辅助训练过程

7.8 表示学习(Representation Learning)

##  VS 2019控制台输出中文乱码问题
https://www.cnblogs.com/yz666/articles/13517505.html
